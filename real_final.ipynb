{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4902a772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train.csv ((17524, 35)) and test.csv ((1000, 35))\n",
      "\n",
      "Class distribution (train):\n",
      "IsFraud\n",
      "0.0    0.772681\n",
      "1.0    0.227319\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution (test):\n",
      "IsFraud\n",
      "0.0    0.67982\n",
      "1.0    0.32018\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Categorical columns (23): ['BankA_BIC', 'BankA_Name', 'Beneficiary_Address', 'Beneficiary_Country', 'Beneficiary_Name', 'Charges', 'CounterpartyRegion', 'Counterparty_BIC', 'Currency', 'Intermediary_BIC', 'LogTimestamp', 'MessageType', 'OrderingCustomer_Address', 'OrderingCustomer_Country', 'OrderingCustomer_Name', 'PurposeCode', 'RandomID', 'RemittanceInfo', 'Status', 'TxnDayOfWeek', 'TxnReference_TRN', 'UETR_UUID', 'ValueDate']\n",
      "Numeric columns (11): ['Amount', 'OrderingCustomer_Account', 'Beneficiary_Account', 'IsHighRiskCountry', 'TxnHour', 'AmountInUSD', 'RelativeAmountToAvg', 'IsRepeatBeneficiary', 'TxnFrequencyLast7Days', 'TotalAmountLast7Days']...\n",
      "Dropping 649 rows with missing target values...\n",
      "\n",
      "Starting RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fraud Detection with RandomForest (Clean & Realistic Pipeline)\n",
    "=============================================================\n",
    "Goal: realistic, generalizable performance — training F1 around 0.75–0.85, test F1 around 0.70–0.80.\n",
    "\n",
    "This notebook:\n",
    "- Handles categorical and numerical preprocessing correctly (no leakage)\n",
    "- Balances classes using class weights (not global oversampling)\n",
    "- Performs randomized hyperparameter search with StratifiedKFold\n",
    "- Evaluates using F1, precision, recall, and ROC-AUC\n",
    "- Searches for the best probability threshold for optimal F1\n",
    "- Saves the best model for reuse\n",
    "\n",
    "Target column: **IsFraud**\n",
    "\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Imports\n",
    "# Import required libraries and set configurations.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "import joblib\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Configuration\n",
    "# Adjust these settings based on your dataset.\n",
    "\n",
    "TRAIN_CSV = 'train.csv'\n",
    "TEST_CSV = 'test.csv'\n",
    "TARGET = 'IsFraud'  # <-- Updated target column name\n",
    "RANDOM_STATE = 42\n",
    "CATEGORICAL_OVERRIDE = None  # e.g. ['merchant','type'] if needed\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Data Loading\n",
    "# Load and combine your train/test datasets or perform a split if only one CSV exists.\n",
    "\n",
    "if os.path.exists(TRAIN_CSV) and os.path.exists(TEST_CSV):\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "    print(f\"Loaded train.csv ({train_df.shape}) and test.csv ({test_df.shape})\")\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "else:\n",
    "    csvs = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "    if not csvs:\n",
    "        raise FileNotFoundError('No CSVs found. Place train.csv/test.csv in the working folder or adjust the script.')\n",
    "    sizes = [(f, os.path.getsize(f)) for f in csvs]\n",
    "    sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "    df = pd.read_csv(sizes[0][0])\n",
    "    print(f\"Loaded {sizes[0][0]} as combined dataframe ({df.shape})\")\n",
    "\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"Target column '{TARGET}' not found in dataframe columns: {df.columns.tolist()}\")\n",
    "\n",
    "if 'train_df' in locals() and 'test_df' in locals():\n",
    "    X_train = train_df.drop(columns=[TARGET])\n",
    "    y_train = train_df[TARGET]\n",
    "    X_test = test_df.drop(columns=[TARGET])\n",
    "    y_test = test_df[TARGET]\n",
    "else:\n",
    "    X = df.drop(columns=[TARGET])\n",
    "    y = df[TARGET]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
    "    print(f\"Split data: X_train {X_train.shape}, X_test {X_test.shape}\")\n",
    "\n",
    "print('\\nClass distribution (train):')\n",
    "print(y_train.value_counts(normalize=True).rename('proportion'))\n",
    "print('\\nClass distribution (test):')\n",
    "print(y_test.value_counts(normalize=True).rename('proportion'))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Feature Preprocessing\n",
    "# Detect categorical/numerical columns and build preprocessing pipeline.\n",
    "\n",
    "if CATEGORICAL_OVERRIDE:\n",
    "    categorical_cols = [c for c in CATEGORICAL_OVERRIDE if c in X_train.columns]\n",
    "else:\n",
    "    categorical_cols = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "    for col in X_train.select_dtypes(include=['int64','int32']):\n",
    "        if X_train[col].nunique() < 20:\n",
    "            categorical_cols.append(col)\n",
    "    categorical_cols = sorted(list(set(categorical_cols)))\n",
    "\n",
    "numeric_cols = [c for c in X_train.columns if c not in categorical_cols]\n",
    "\n",
    "print(f\"\\nCategorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Numeric columns ({len(numeric_cols)}): {numeric_cols[:10]}{'...' if len(numeric_cols)>10 else ''}\")\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Model and Hyperparameter Search\n",
    "# Set up the RandomForest classifier with balanced class weights and search for optimal hyperparameters.\n",
    "\n",
    "rf = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1, class_weight='balanced')\n",
    "\n",
    "pipeline = Pipeline(steps=[('preproc', preprocessor), ('clf', rf)])\n",
    "\n",
    "param_dist = {\n",
    "    'clf__n_estimators': sp_randint(150, 500),\n",
    "    'clf__max_depth': sp_randint(6, 16),\n",
    "    'clf__min_samples_split': sp_randint(4, 12),\n",
    "    'clf__min_samples_leaf': sp_randint(2, 8),\n",
    "    'clf__max_features': ['sqrt', 'log2', 0.3, 0.5]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=40,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "# Drop rows where target is NaN\n",
    "train_na_rows = y_train[y_train.isna()].index\n",
    "if len(train_na_rows) > 0:\n",
    "    print(f\"Dropping {len(train_na_rows)} rows with missing target values...\")\n",
    "    X_train = X_train.drop(index=train_na_rows)\n",
    "    y_train = y_train.drop(index=train_na_rows)\n",
    "\n",
    "\n",
    "\n",
    "print('\\nStarting RandomizedSearchCV...')\n",
    "rs.fit(X_train, y_train)\n",
    "\n",
    "print('\\nBest parameters:')\n",
    "print(rs.best_params_)\n",
    "print('\\nBest CV F1: {:.4f}'.format(rs.best_score_))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Model Evaluation\n",
    "# Evaluate model on the test set, optimize threshold for best F1, and report results.\n",
    "\n",
    "best = rs.best_estimator_\n",
    "y_pred_proba = best.predict_proba(X_test)[:,1]\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 33)\n",
    "metrics = []\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_pred_proba >= t).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred_t, zero_division=0)\n",
    "    precision = precision_score(y_test, y_pred_t, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred_t, zero_division=0)\n",
    "    metrics.append((t, f1, precision, recall))\n",
    "\n",
    "best_thresh, best_f1, best_prec, best_rec = max(metrics, key=lambda x: x[1])\n",
    "print(f\"\\nBest threshold: {best_thresh:.3f} | F1: {best_f1:.4f}, Precision: {best_prec:.4f}, Recall: {best_rec:.4f}\")\n",
    "\n",
    "y_pred_best = (y_pred_proba >= best_thresh).astype(int)\n",
    "\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred_best, zero_division=0))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred_best))\n",
    "print('ROC-AUC:', roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Feature Importances\n",
    "# Extract and visualize the most important features contributing to the prediction.\n",
    "\n",
    "preproc = best.named_steps['preproc']\n",
    "clf = best.named_steps['clf']\n",
    "\n",
    "num_features = numeric_cols\n",
    "cat_features = []\n",
    "if categorical_cols:\n",
    "    ohe = preproc.named_transformers_['cat'].named_steps['ohe']\n",
    "    cat_names = []\n",
    "    for i, col in enumerate(categorical_cols):\n",
    "        cats = ohe.categories_[i]\n",
    "        names = [f\"{col}__{str(c)}\" for c in cats]\n",
    "        cat_names.extend(names)\n",
    "    cat_features = cat_names\n",
    "\n",
    "feature_names = list(num_features) + list(cat_features)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "if len(importances) == len(feature_names):\n",
    "    fi = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "    print('\\nTop 30 Feature Importances:')\n",
    "    display(fi.head(30))\n",
    "else:\n",
    "    print('Feature importance alignment issue — skipping.')\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Save Best Model\n",
    "# Save the tuned RandomForest model for later use.\n",
    "\n",
    "model_path = 'rf_best_model.joblib'\n",
    "joblib.dump(rs.best_estimator_, model_path)\n",
    "print(f\"Model saved as {model_path}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. Next Steps\n",
    "# - If test F1 < 0.7, perform feature engineering (e.g., ratios, time-based features).\n",
    "# - Try SMOTE *inside* cross-validation (never globally).\n",
    "# - After RandomForest stabilizes, experiment with XGBoost or LightGBM.\n",
    "# - Always evaluate on a separate unseen set to confirm generalization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
